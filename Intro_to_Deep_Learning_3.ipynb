{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_Deep_Learning_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "cell_type": "markdown",
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mukul-rathi/workshop-deep-learning/blob/master/LSTM.ipynb)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "S3NOjcDJun6V"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to Deep Learning 3\n",
        "\n",
        "This is the notebook accompanying the second Hackers at Cambridge Deep Learning workshop.\n",
        "In this workshop, you'll implement your own *LSTM neural network* using the **Keras** deep learning framework.\n"
      ]
    },
    {
      "metadata": {
        "id": "U5ViC3zU0d8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First let's import dependencies:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8_IsUj_aulCK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#import the keras functions\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import  Input, Embedding, Dense, LSTM\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "#import the IMDB dataset\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zrqO7kqGvCNU"
      },
      "cell_type": "markdown",
      "source": [
        "## Reading In the Data:\n",
        "\n",
        "As in the first and second workshops, you'll want to load in the data, using a nice **load data()** function.\n",
        "\n",
        "We'll be loading in the movie reviews from the IMDB dataset. `num_words` is a parameter that specifies the number of words our lexicon should contain. \n",
        "\n",
        "`x_train / x_test` contain the reviews, and `y_train / y_test` contain the binary labels (0 = negative 1 = positive).\n",
        "\n",
        "Since some reviews can be very long and so take long to train on, we want to clip the number of words to a maximum length (here we have chosen 200 words since most of the reviews are less than 200 words). \n",
        "\n",
        "**EXTENSION:** Look at the distribution of the lengths of the reviews and change the maxlen accordingly to encompass more of the reviews.\n",
        "\n",
        "### Useful Functions:\n",
        "\n",
        "    sequence.pad_sequences(input, maxlen).\n",
        "    "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h82QS2d_vA_G",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = 20000\n",
        "maxlen = 200\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "\n",
        "#Preprocess the data so the same length (padding as necessary)\n",
        "x_train = None\n",
        "x_test = None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FP9oDQMk0d9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the model:\n",
        "          \n",
        "Keras is a high level deep learning framework that makes it really easy to create our own model - in this workshop we will be using the **Sequential API**. \n",
        "\n",
        "There are four steps to creating a model in Keras:\n",
        " * Define \n",
        " * Compile\n",
        " * Fit\n",
        " * Evaluate\n",
        " \n",
        " \n",
        " We'll look at each stage in turn:"
      ]
    },
    {
      "metadata": {
        "id": "OD7fibRe0d9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the model:\n",
        "\n",
        "This is where we specify the layers in our model.\n",
        "\n",
        "We are using the Sequential API so to start we'll define the model:\n",
        "\n",
        "      model = Sequential()\n",
        "      \n",
        "This is the base to start on. We can then sequentially add the layers to the model using \n",
        "\n",
        "            model.add(layer object)\n",
        "      \n",
        " e.g. `Dense()` - this is a fully-connected layer\n",
        " \n",
        " so you might have:\n",
        " \n",
        "          model = Sequential()\n",
        "          model.add(Dense(units=128, activation='relu'))\n",
        "          model.add(Dense(units=10, activation='softmax'))\n",
        " \n",
        " and now  *model* would be a 2 layer neural network.\n",
        " \n",
        " The layers take in different arguments - in the example above Dense() took arguments the number of neurons (128, 10 respectively) and the activation function used. \n",
        " \n",
        "For our LSTM we'll need 3 different layers:\n",
        "\n",
        "* [Embeddings](https://keras.io/layers/embeddings/) - this will map each of the words in the lexicon to its word embedding. It takes two arguments `(input dim, output_dim)` - the input dim will be the number of words, and the output dim we can set arbitrarily to 128 (the length of the word vector. )\n",
        "\n",
        "* [LSTM](https://keras.io/layers/recurrent/) - this is the recurrent part of the model itself - it will take parameters the `input_dim` (the length of the word vectors) and an additional parameter called `recurrent_dropout = 0.2` (we set the value 0.2 to prevent the model *overfitting*).\n",
        "\n",
        "* [Dense](https://keras.io/layers/core/#dense) has arguments `units=n`  (number of neurons in layer) `activation = __ `  (`'relu', 'softmax', 'sigmoid', 'tanh'` are possible activations). We want one class (positive/negative) - so one neuron, and since it is a probability, we'd like the sigmoid function.\n",
        "      "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i2QEMfqFvajF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initLSTM():\n",
        "  lstm = None #fill in the layers\n",
        "  return lstm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MDx9oKxFvNyx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm = initLSTM()\n",
        "lstm.summary() #print the description of the layers in the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yU_rceWVvgqJ"
      },
      "cell_type": "markdown",
      "source": [
        "### Compiling the Model:\n",
        "\n",
        "This is where we specify the loss function we are using, the optimizer we are using and the metrics we want to track when training, as well as a bunch of other information pertaining to training.\n",
        "\n",
        "        model.compile(arguments)\n",
        "        \n",
        " where the arguments we are interested in are\n",
        " \n",
        "      loss='binary_crossentropy',\n",
        "      optimizer='adam',\n",
        "       metrics=['accuracy']\n",
        "       \n",
        "       \n",
        "  `binary_crossentropy` refers to our loss function for probability of pos/neg sentiment. \n",
        "  \n",
        " `'adam'` refers to the Adam optimiser, a variant of gradient descent which allows for much faster convergence to the minimum.\n",
        "  "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-Ll4K56DvIYU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Extension: try compiling using different optimizers and different optimizer configs\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gSxfKQWl4Keo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fitting the model\n",
        "\n",
        "This is where we actually train our model, feel free to alter the hyperparameters (e.g. number of epochs to train it for longer etc.)\n",
        "\n",
        "      model.fit(x=, y=, epochs=, batch_size=, validation_data=(x_test, y_test))\n",
        "\n",
        "x, y are the training set\n",
        "\n",
        "The validation_data argument takes as input the **validation dataset**. Here we have split the dataset into train:test split, however typically we would split into train:validation:test - and the purpose of the validation data is to tune the hyperparameters (so we try to maximise performance on the validation data set).\n",
        "\n",
        "For today, we'll just use x_test, y_test, though I encourage you (as an extension) to split the data up so you have a separate validation set.\n"
      ]
    },
    {
      "metadata": {
        "id": "3ffT-zDq4PZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fit the model \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_t1-yOqR4Zqo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model\n",
        "\n",
        "Finally, we can use the model.evaluate function to evaluate how well the model has done on the test set. Here x, y are the test set inputs/labels respectively.\n",
        "\n",
        "        loss, accuracy = model.evaluate(x=, y=)\n",
        "        \n",
        "   "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wt6nprr1wBml",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss, acc = None\n",
        "print(\"Test accuracy: \" + str(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FOMuE1BM86I-"
      },
      "cell_type": "markdown",
      "source": [
        "## See specific predictions:\n",
        "\n",
        "If you're curious to see how the model performs on random reviews from the test set, run the following cells.\n",
        "\n",
        "We need a print_review function, since the vector x_train actually contains the *ids* of the words in the lexicon, not the words themselves.\n",
        "\n",
        "We map the ids to the words, and then iterate through the review to print the overall review.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hkjWBEzMxitT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#create a mapping from the index to the word\n",
        "idx_to_word = {(v+3):k for k,v in imdb.get_word_index().items()}\n",
        "idx_to_word.update({0:\"<PAD>\", 1: \"<START>\", 2: \"<UNK>\",3:\"<UNUSED>\"}) #first 3 indices are special tokens \n",
        "\n",
        "vocab_size = np.max(list(idx_to_word.keys()))\n",
        "\n",
        "\n",
        "#this is a helper function - good to debug performance of model during training\n",
        "def print_review(x):\n",
        "    text = \"\"\n",
        "    for idx in x:\n",
        "        text += idx_to_word.get(idx, \"<UNK>\") + \" \" #if word not in dictionary it is unknown\n",
        "    print(text)\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zQ7JF_Pj82Fb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#choose a random review \n",
        "review_num = np.random.randint(0,x_test.shape[0])\n",
        "review = x_test[review_num]\n",
        "review = np.reshape(review, (1, x_test.shape[1]))\n",
        "\n",
        "print(\"The predicted sentiment is: \" + str((lstm.predict(review))))\n",
        "print_review(review[0])\n",
        "print(\"The actual sentiment is: \" + str(y_test[review_num]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}