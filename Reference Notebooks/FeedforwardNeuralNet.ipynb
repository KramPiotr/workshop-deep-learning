{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeedforwardNeuralNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "R9Dcw7yC_FTM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "This notebook accompanies the Intro to Deep Learning workshop run by Hackers at Cambridge"
      ]
    },
    {
      "metadata": {
        "id": "0FSW41K5_FTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing Data and Dependencies\n",
        "\n",
        "\n",
        "First, we will import the dependencies - **numpy**, the python linear algebra library, **pandas** to load and preprocess the input data and **matplotlib** for visualisation purposes.\n",
        "\n",
        "We'll import the datasets using nice loader functions from **sklearn**."
      ]
    },
    {
      "metadata": {
        "id": "w2tsb3eK_FTP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_boston,load_breast_cancer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ozNhjbuS_FTT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll load the house dataset, using the load_boston(return_X_y=True) function. This returns our inputs $X$ and our labels $y$ as a tuple.\n",
        "If you read them in, you'll notice they're not the right dimensions, so you'll need to reshape them.\n",
        "\n",
        "It is also good practice to normalise the data (subtract mean and then divide by standard deviation) as this speeds up learning.\n",
        "\n",
        "Finally, split the data into training and test data sets (80:20 split) - we'll keep the test data to the side to evaluate our model at the end."
      ]
    },
    {
      "metadata": {
        "id": "XUe4J9h8BRwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X,y = load_boston(return_X_y=True)\n",
        "#X,y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# make sure dimensions are right\n",
        "X = X.T\n",
        "y = np.reshape(y, (1,y.shape[0]))\n",
        "\n",
        "\n",
        "#normalise the data\n",
        "mean = np.mean(X, axis=1, keepdims=True)\n",
        "std = np.std(X, axis=1,keepdims = True)\n",
        "X -=mean\n",
        "X /= std\n",
        "\n",
        "#split data into train and test set\n",
        "X_train = X[:,:4*X.shape[0]//5]\n",
        "Y_train = y[:,:4*X.shape[0]//5]\n",
        "\n",
        "X_test = X[:,4*X.shape[0]//5:]\n",
        "Y_test = y[:,4*X.shape[0]//5:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pe4zmPEu_FTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the neural network:\n",
        "\n",
        "Having preprocessed our data into matrices, it is now time to create the feedforward neural network. "
      ]
    },
    {
      "metadata": {
        "id": "sUfdUeJQ_FTd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First we need to initialise parameters: the weights and biases for each layer.\n",
        "\n",
        "The weights for layer *$l$* are stored in *$ W^{[l]}$*, a *$n_l$ x $n_{(l-1)}$* matrix, where *$n_l$* is the number of units in layer *$l$*. \n",
        "We  initialise the weights randomly to break symmetry, and multiply by 0.001 to ensure weights aren't too large.\n",
        "\n",
        "The biases for layer *$l$* are stored in *$ b^{[l]}$*, which is a *$n_l$ x 1* matrix."
      ]
    },
    {
      "metadata": {
        "id": "fQI5mNln_FTf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialise_parameters(layers_units):\n",
        "    parameters = {}            # create a dictionary containing the parameters\n",
        "    for l in range(1, len(layers_units)):\n",
        "        parameters['W' + str(l)] = 0.001* np.random.randn(layers_units[l],layers_units[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layers_units[l],1))\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBvP-WeS_FTi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The activation function $g(z)$ we will be using is the ReLU function $g(z) = max(0,z)$ in the hidden layers.\n",
        "\n",
        "\n",
        "NB: Although the ReLU function is technically non-differentiable when $z=0$, in practice we can set the derivative=0 at $z=0$."
      ]
    },
    {
      "metadata": {
        "id": "-zS22wKC_FTj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def relu(z, deriv = False):\n",
        "    if(deriv):  \n",
        "        return z>0\n",
        "    else:\n",
        "        return np.multiply(z, z>0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xdw2iwNX_FTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now write the code for the forward propagation step.\n",
        "\n",
        "In each layer $l$ , we matrix multiply the output of the previous layer $A^{(l-1)}$  by a weight matrix $W^{(l)}$ and then add a bias term $b^{(l)}$. We then take the result $Z^{(l)}$ and apply the activation function $g(z)$ to it to get the output $A^{(l)}$. $L$ = number of layers.\n",
        "\n",
        "The equations are thus:\n",
        "$$Z^{(l)}=W^{[l]}A^{([l-1]} + b^{[l]}$$\n",
        "$$A^{[l]}=g(Z^{[l]})$$\n"
      ]
    },
    {
      "metadata": {
        "id": "G1mGl8Ol_FTl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(X,parameters):\n",
        "    cache = {}\n",
        "    L = len(parameters)//2 #final layer\n",
        "    cache[\"A0\"] = X #ease of notation since input = layer 0\n",
        "    for l in range(1, L):\n",
        "        cache['Z' + str(l)] = np.dot(parameters['W' + str(l)],cache['A' + str(l-1)]) + parameters['b' + str(l)]\n",
        "        cache['A' + str(l)] = relu(cache['Z' + str(l)])\n",
        "    #final layer\n",
        "    cache['Z' + str(L)] = np.dot(parameters['W' + str(L)],cache['A' + str(L-1)]) + parameters['b' + str(L)]\n",
        "    cache['A' + str(L)] =cache['Z' + str(L)] \n",
        "    return cache "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwuBdFF9_FTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing the Learning \n",
        "\n",
        "\n",
        "$m$ = number of training examples, $(x^{(i)},y^{(i)})$ is the $i^{th}$ training example, $a^{[L](i)}$ is the output of the final layer $L$ for that $i^{th}$ training example.\n",
        "\n",
        "\n",
        "**Mean Squared Error:**\n",
        "\n",
        "$$ J(W^{(1)}, b^{(1)},...) = \\frac{1}{2m} \\sum_{i=1}^{m} (a^{[L](i)} - y^{(i)})^2 $$\n"
      ]
    },
    {
      "metadata": {
        "id": "h614AF-__FTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cost_function(AL,Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = (1/(2*m))*(np.sum(np.square(AL-Y)))\n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GTMg3uoB_FTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backpropagation:\n",
        "\n",
        "Calculating the gradients:\n",
        "\n",
        "For the final layer:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{J} }{\\partial Z^{(L)}} = A^{(L)} - Y$$ \n",
        "\n",
        "\n",
        "For a general layer $l$, \n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{J} }{\\partial A^{[l]}}*g^{'}(Z^{[l]})$$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m}\\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}} A^{[l-1] T} $$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial b^{(l)}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{J} }{\\partial Z^{(l)(i)}}$$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial A^{[l-1]}} = W^{[l] T} \\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}} $$\n",
        "\n",
        "<br>\n",
        "\n",
        "If you are keen, it's a good exercise to derive them yourself or alternatively check this [post](https://mukul-rathi.github.io/2018/08/31/Backpropagation.html) for a deeper dive into the intuition behind it.\n"
      ]
    },
    {
      "metadata": {
        "id": "JMoNeqUU_FTt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backpropagation(cache,Y,parameters):\n",
        "    L = len(parameters)//2 \n",
        "    m = Y.shape[1]\n",
        "    grads = {}\n",
        "    #code up the last layer explicitly \n",
        "    grads[\"dZ\" + str(L)]= cache[\"A\" + str(L)] - Y\n",
        "    grads[\"dW\" + str(L)]= (1/m)*np.dot(grads[\"dZ\" + str(L)],cache[\"A\" + str(L-1)].T) \n",
        "    grads[\"db\" + str(L)]= (1/m)*np.sum(grads[\"dZ\" + str(L)],axis=1,keepdims=True)\n",
        "    for l in range(L-1,0,-1):\n",
        "        grads[\"dA\" + str(l)]= np.dot(parameters[\"W\" + str(l+1)].T,grads[\"dZ\" + str(l+1)])\n",
        "        grads[\"dZ\" + str(l)]= np.multiply(grads[\"dA\" + str(l)], relu(cache[\"Z\" + str(l)], deriv = True))\n",
        "        grads[\"dW\" + str(l)]= (1/m)*np.dot(grads[\"dZ\" + str(l)],cache[\"A\" + str(l-1)].T) \n",
        "        grads[\"db\" + str(l)]= (1/m)*np.sum(grads[\"dZ\" + str(l)],axis=1,keepdims=True)\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "FtDypLdm_FTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "Now let's combine the functions created so far to create a model and train it using  gradient descent. \n",
        "\n",
        "\n",
        "The update equations for the parameters are as follows:\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} $$\n",
        "\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} $$\n",
        "\n",
        "where $\\alpha$ is the learning rate parameter."
      ]
    },
    {
      "metadata": {
        "id": "DPXbsDfq_FTz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(X_train, Y_train,num_epochs,layers_units,learning_rate): #epoch = one cycle through the dataset\n",
        "    train_costs = []\n",
        "    \n",
        "    parameters = initialise_parameters(layers_units)\n",
        "    L = len(layers_units)-1 \n",
        "    for epoch in range (num_epochs):\n",
        "        #perform one cycle of forward and backward propagation to get the partial derivatives w.r.t. the weights\n",
        "        #and biases. Calculate the cost - used to monitor training\n",
        "        cache = forward_propagation(X_train,parameters)\n",
        "        cost = cost_function(cache[\"A\" + str(L)],Y_train)\n",
        "        grads = backpropagation(cache,Y_train,parameters)\n",
        "\n",
        "        #iterate through and update the parameters using gradient descent\n",
        "        for l in range(1,L+1):\n",
        "            parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*grads[\"dW\" + str(l)]\n",
        "            parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*grads[\"db\" + str(l)]\n",
        "\n",
        "        #periodically output an update on the current cost and performance on the dev set for visualisation\n",
        "        train_costs.append(cost)\n",
        "        if(epoch%(num_epochs//10)==0):\n",
        "            print(\"Training the model, epoch: \" + str(epoch+1))\n",
        "            print(\"Cost after epoch \" + str((epoch)) + \": \" + str(cost))\n",
        "    print(\"Training complete!\")\n",
        "    #return the trained parameters and the visualisation metrics\n",
        "    return parameters, train_costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhvigzxs_FT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To evaluate the model, we'll visualise the training set error over the number of iterations. We then output the final value of the evaluation metric for training and test sets. (I've used *matplotlib* to plot the graph)."
      ]
    },
    {
      "metadata": {
        "id": "z6Pfr4gB_FT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(train_costs,parameters,X_train, Y_train, X_test, Y_test):\n",
        "    #plot the graphs of training set error\n",
        "    plt.plot(np.squeeze(train_costs))\n",
        "    plt.ylabel('Cost')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.title(\"Training Set Error\")\n",
        "    plt.show()\n",
        "    L = len(parameters)//2\n",
        "    \n",
        "    #For train and test sets, perform a step of forward propagation to obtain the trained model's \n",
        "    #predictions and evaluate this\n",
        "    \n",
        "    train_cache = forward_propagation(X_train,parameters)\n",
        "    train_AL = train_cache[\"A\"+ str(L)]\n",
        "    \n",
        "    print(\"The train set MSE is: \"+str(cost_function(train_AL,Y_train)))\n",
        "        \n",
        "    test_cache = forward_propagation(X_test,parameters)\n",
        "    test_AL = test_cache[\"A\"+ str(L)]\n",
        "    \n",
        "    print(\"The test set MSE is: \"+str(cost_function(test_AL,Y_test)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qHYA2ql8kX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "Now it's time to train the model using our helper functions.\n",
        "\n",
        "Let's define our hyperparameters - I encourage you to play around with these - e.g. add more layers, change number of iterations.\n",
        "\n",
        "You might find the model does much worse on the test set - this is called **overfitting** - again you can read up more about it [here](https://mukul-rathi.github.io/2018/09/02/DebuggingLearningCurve.html)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vQwKmiqV_FT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define the hyperparameters for the model\n",
        "num_epochs = 1500 #number of passes through the training set\n",
        "layers_units = [X.shape[0], 1] #layer 0 is the input layer - each value in list = number of nodes in that layer\n",
        "learning_rate = 1e-4\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "few8mbeH_FT8",
        "colab_type": "code",
        "outputId": "d1f75e8b-f9e2-4100-b5c1-80e1b8ae3266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "parameters, train_costs = train_model(X_train, Y_train ,num_epochs,layers_units,learning_rate)         "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model, epoch: 1\n",
            "Cost after epoch 0: 422.1854325453298\n",
            "Training the model, epoch: 151\n",
            "Cost after epoch 150: 407.5641428446872\n",
            "Training the model, epoch: 301\n",
            "Cost after epoch 300: 393.5574120947215\n",
            "Training the model, epoch: 451\n",
            "Cost after epoch 450: 380.14019939125455\n",
            "Training the model, epoch: 601\n",
            "Cost after epoch 600: 367.2884871287632\n",
            "Training the model, epoch: 751\n",
            "Cost after epoch 750: 354.9792381524502\n",
            "Training the model, epoch: 901\n",
            "Cost after epoch 900: 343.1903547592035\n",
            "Training the model, epoch: 1051\n",
            "Cost after epoch 1050: 331.90063946539135\n",
            "Training the model, epoch: 1201\n",
            "Cost after epoch 1200: 321.08975746316537\n",
            "Training the model, epoch: 1351\n",
            "Cost after epoch 1350: 310.73820069050515\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HxRFpsk0Kd9D",
        "colab_type": "code",
        "outputId": "2b059dca-42ca-47bd-9a18-d80eb5d5bfe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "cell_type": "code",
      "source": [
        "evaluate_model(train_costs,parameters,X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFnCAYAAABdOssgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f810ee57630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The train set MSE is: 300.8272534436269\n",
            "The test set MSE is: 308.07231173135284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2U7b7ebN9cRP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary and Extensions:\n",
        "\n",
        "## Summary and Extensions:\n",
        "\n",
        "You've just trained your first deep learning model! As an extension, try running the code again, but this time, use the **load_cancer()** function instead of **load boston()**. This is a dataset that classifies breast cancer as malignant/benign.\n",
        "\n",
        "Remember that sigmoid function in the lectures? We can use it to predict probabilities for classification, so all you need to do is apply it to the output of the final layer.\n",
        "\n",
        "A couple of other minor tweaks - for classification, the network uses the **cross-entropy loss** as a cost function instead of mean-square error, and you'll want to print out accuracy not MSE in the evaluation function. \n",
        "\n",
        "But the cool thing is that the network structure is the **same**! The same network, just with a sigmoid function applied to the output, can be trained on a *completely different task* and still work.\n",
        "\n",
        "That's the power of deep learning! Stay tuned for future workshops on specialised deep learning models for computer vision and natural language processing. If you want to dive deeper, head over to the [blog](http://mukul-rathi.github.io/blog.html)."
      ]
    },
    {
      "metadata": {
        "id": "SI2k1ZTt_PRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}