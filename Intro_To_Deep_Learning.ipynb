{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro To Deep Learning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "R9Dcw7yC_FTM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to Deep Learning\n",
        "\n",
        "This notebook accompanies the Intro to Deep Learning workshop run by Hackers at Cambridge\n",
        "\n",
        "TODO: Fill in the code corresponding the comments, and replace the variables set to None."
      ]
    },
    {
      "metadata": {
        "id": "0FSW41K5_FTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing Data and Dependencies\n",
        "\n",
        "\n",
        "First, we will import the dependencies - **numpy**, a python library we'll be using for matrix multplication, and **matplotlib** for visualisation purposes.\n",
        "\n",
        "We'll import the datasets using nice loader functions from **sklearn**.\n",
        "\n",
        "\n",
        "To run a cell, hover over the [    ] next to each cell and click the play button on the left - the number in the [   ]  tells you the order of execution (1, 2, 3 etc.)"
      ]
    },
    {
      "metadata": {
        "id": "w2tsb3eK_FTP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_boston,load_breast_cancer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ozNhjbuS_FTT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll load the house dataset, using the **load_boston(return_X_y=True)** function. This returns our inputs $X$ and our labels $y$ as a tuple.\n",
        "If you read them in, you'll notice they're not the right dimensions, so you'll need to reshape them. We have $m=506$ examples and $n=13$ features. \n",
        "\n",
        "We want to store $X$ in a $n$ x $m$ matrix (currently it is $m$ x $n$), and $y$ in a $1$ x $m$ matrix - currently it is a $m$ dimensional vector\n",
        "\n",
        "\n",
        "It is also good practice to normalise the data: $\\mu$ = mean, $\\sigma$ = standard deviation *across examples*.\n",
        "\n",
        "$$X = \\frac{X-\\mu}{\\sigma}$$\n",
        "\n",
        "as this speeds up learning.\n",
        "\n",
        "Finally, split the data into training and test data sets (80:20 split)- we'll keep the test data to the side to evaluate our model at the end.\n",
        "\n",
        "Rather than hardcoding dimensions like 506, 13 etc, extract the dimensions from the X.shape tuple and use them - this will allow you to reuse the code for different datasets.\n",
        "\n",
        "### Useful functions:\n",
        "\n",
        "          A.shape #return a tuple consisting of A's dimensions\n",
        "          \n",
        "          A = np.reshape(A, (b,c)) # reshapes A into a b x c matrix - (b,c) = tuple of dimensions\n",
        "          A.T  #returns the transpose of A (flips rows and columns)\n",
        "          \n",
        "          np.mean(A, axis=1, keepdims=True) # takes the mean of A across axis 1, keepdims=True ensures number of dimensions is same. if we have a (a,b) dimensional array then using axis=0 returns a (1,b) dimensional array\n",
        "          \n",
        "                   \n",
        "         np.std(A, axis=1, keepdims=True) #ditto but with standard deviation\n",
        "       \n",
        "       A[2:5, :] # we can take slices - e.g. this will return all the columns of the rows 2-4 \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XUe4J9h8BRwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X,y = load_boston(return_X_y=True)\n",
        "\n",
        "# reshape to make sure dimensions are right \n",
        "X = None\n",
        "y = None\n",
        "\n",
        "\n",
        "#normalise the data\n",
        "X = None\n",
        "\n",
        "#split data into train and test set\n",
        "X_train = None\n",
        "Y_train = None\n",
        "\n",
        "X_test = None\n",
        "Y_test = None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pe4zmPEu_FTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the neural network:\n",
        "\n",
        "Having preprocessed our data into matrices, it is now time to create the feedforward neural network. "
      ]
    },
    {
      "metadata": {
        "id": "sUfdUeJQ_FTd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First we need to initialise parameters: the weights and biases for each layer.\n",
        "\n",
        "The weights for layer *$l$* are stored in *$ W^{[l]}$*, a *$n_l$ x $n_{(l-1)}$* matrix, where *$n_l$* is the number of units in layer *$l$*. \n",
        "We  initialise the weights randomly to break symmetry, and multiply by 0.001 to ensure weights aren't too large.\n",
        "\n",
        "The biases for layer *$l$* are stored in *$ b^{[l]}$*, which is a *$n_l$ x 1* matrix.\n",
        "\n",
        "### Useful functions:\n",
        "          \n",
        "          np.random.randn(a, b) # creates a random matrix with dimensions (a,b)\n",
        "          np.zeros((a,b))  #matrix of zeros of size (a,b) - note the extra set of brackets!"
      ]
    },
    {
      "metadata": {
        "id": "fQI5mNln_FTf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialise_parameters(layers_units): #layers_units = list of number of nodes in each layer\n",
        "    parameters = {}            # create a dictionary containing the parameters\n",
        "    for l in range(1, len(layers_units)):\n",
        "        parameters['W' + str(l)] = None\n",
        "        parameters['b' + str(l)] = None\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBvP-WeS_FTi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Activation Functions:\n",
        "\n",
        "The activation function $g(z)$ we will be using is the ReLU function $g(z) = max(0,z)$ in the hidden layers.\n",
        "\n",
        "Another one is sigmoid : $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "NB: Although the ReLU function is technically non-differentiable when $z=0$, in practice we can set the derivative=0 at $z=0$.\n",
        "\n",
        "\n",
        "###Useful functions:\n",
        "      \n",
        "      np.exp(z) #exponentiates z element-wise\n",
        "      A>c  #compares each element of A with c and because this is Python, False=0 , True=1 when multiplying with ints.\n",
        " \n",
        " So if you multiply this with matrix **A** , you can zero-out values where A>c is false."
      ]
    },
    {
      "metadata": {
        "id": "-zS22wKC_FTj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return None\n",
        "\n",
        "def relu(z, deriv = False):\n",
        "    if(deriv):  #This is for gradients - do this when you do next section!\n",
        "        return None #gradient = 1 if z>0, 0 otherwise\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xdw2iwNX_FTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now write the code for the forward propagation step.\n",
        "\n",
        "In each layer $l$ , we matrix multiply the output of the previous layer $A^{(l-1)}$  by a weight matrix $W^{(l)}$ and then add a bias term $b^{(l)}$. We then take the result $Z^{(l)}$ and apply the activation function $g(z)$ to it to get the output $A^{(l)}$. $L$ = number of layers.\n",
        "The equations are thus:\n",
        "$$Z^{(l)}=W^{[l]}A^{([l-1]} + b^{[l]}$$\n",
        "$$A^{[l]}=g(Z^{[l]})$$\n",
        " \n",
        "Here, we have $g(z) = ReLU(z)$\n",
        "\n",
        "### Useful functions:\n",
        "          \n",
        "          C = A.dot(B) #matrix multiplies A, B\n",
        "          C = np.dot(A,B) #equivalent operation\n",
        "          \n",
        "         \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G1mGl8Ol_FTl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(X,parameters):\n",
        "    cache = {} #stores all our intermediate calculations since \n",
        "    L = len(parameters)//2 #final layer\n",
        "    cache[\"A0\"] = X #ease of notation since input = layer 0\n",
        "    for l in range(1, L):\n",
        "        cache['Z' + str(l)] = None\n",
        "        cache['A' + str(l)] = None #use relu as activation function\n",
        "    #final layer\n",
        "    cache['Z' + str(L)] = None\n",
        "    cache['A' + str(L)] = cache['Z' + str(L)] #no activation function for last layer \n",
        "    return cache "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwuBdFF9_FTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing the Learning \n",
        "\n",
        "\n",
        "Next we can compute the loss function - this is the objective function the neural network will aim to minimise during training:\n",
        "\n",
        "$m$ = number of training examples, $(x^{(i)},y^{(i)})$ is the $i^{th}$ training example, $a^{[L](i)}$ is the output of the final layer $L$ for that $i^{th}$ training example.\n",
        "\n",
        "\n",
        "**Mean Squared Error:**\n",
        "\n",
        "$$ J(W^{(1)}, b^{(1)},...) = \\frac{1}{2m} \\sum_{i=1}^{m} (a^{[L](i)} - y^{(i)})^2 $$\n",
        "\n",
        "### Useful functions:\n",
        "          \n",
        "         np.square(A) #square each element in A\n",
        "\n",
        "          np.sum(A, axis=1, keepdims= True) # just like mean and std, take sum along axis 1\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "h614AF-__FTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cost_function(AL,Y):\n",
        "    m = None\n",
        "    cost = None\n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GTMg3uoB_FTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backpropagation:\n",
        "\n",
        "Calculating the gradients:\n",
        "\n",
        "For the final layer:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{J} }{\\partial Z^{(L)}} = A^{(L)} - Y$$ \n",
        "\n",
        "\n",
        "For a general layer $l$, \n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{J} }{\\partial A^{[l]}}*g^{'}(Z^{[l]})$$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m}\\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}} A^{[l-1] T} $$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial b^{(l)}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{J} }{\\partial Z^{(l)(i)}}$$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{J} }{\\partial A^{[l-1]}} = W^{[l] T} \\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}} $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If you are keen, it's a good exercise to derive them yourself or alternatively check this [post](https://mukul-rathi.github.io/2018/08/31/Backpropagation.html) for a deeper dive into the intuition behind it.\n",
        "\n",
        "### Useful functions:\n",
        "          np.sum(A, axis=1, keepdims= True) # just like mean and std, take sum along axis 1\n",
        "          \n",
        "          A.dot(B) # matrix multiplication returns A.B\n",
        "          \n",
        "         A*B #returns elementwise multiplication (useful for last equation)\n",
        "​\n",
        "​"
      ]
    },
    {
      "metadata": {
        "id": "JMoNeqUU_FTt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backpropagation(cache,Y,parameters):\n",
        "    L = len(parameters)//2 \n",
        "    m = Y.shape[1]\n",
        "    grads = {}\n",
        "    #code up the last layer explicitly\n",
        "    grads[\"dZ\" + str(L)]= None\n",
        "    grads[\"dW\" + str(L)]= None\n",
        "    grads[\"db\" + str(L)]= None\n",
        "    for l in range(L-1,0,-1): \n",
        "        grads[\"dA\" + str(l)]= None\n",
        "        grads[\"dZ\" + str(l)]= None\n",
        "        grads[\"dW\" + str(l)]= None\n",
        "        grads[\"db\" + str(l)]= None\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "FtDypLdm_FTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "Now let's combine the functions created so far to create a model and train it using  gradient descent. \n",
        "\n",
        "The update equations for the parameters are as follows:\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} $$\n",
        "\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} $$\n",
        "\n",
        "where $\\alpha$ is the learning rate parameter."
      ]
    },
    {
      "metadata": {
        "id": "DPXbsDfq_FTz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(X_train, Y_train,num_epochs,layers_units,learning_rate): #epoch = one cycle through the dataset\n",
        "    train_costs = []\n",
        "    \n",
        "    #Initialise the parameters\n",
        "    \n",
        "    L = len(layers_units)-1 \n",
        "    for epoch in range (num_epochs):\n",
        "        #run one step of forward propagation\n",
        "        \n",
        "        #calculate the cost\n",
        "        \n",
        "        #get gradients using backpropagation\n",
        "        grads = None\n",
        "\n",
        "        #iterate through each layer and update the parameters using gradient descent \n",
        "        #hint:  weight at layer l  = parameters[\"W\"+ str(l)]  \n",
        "        for _ in _:\n",
        "            None\n",
        "\n",
        "        #periodically output an update on the current cost and performance on the training set for visualisation\n",
        "        train_costs.append(cost)\n",
        "        if(epoch%(num_epochs//10)==0):\n",
        "            print(\"Training the model, epoch: \" + str(epoch+1))\n",
        "            print(\"Cost after epoch \" + str((epoch)) + \": \" + str(cost))\n",
        "    print(\"Training complete!\")\n",
        "    #return the trained parameters and the visualisation metrics\n",
        "    return parameters, train_costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhvigzxs_FT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To evaluate the model, we'll visualise the training set error over the number of iterations. We then output the final value of the evaluation metric for training and test sets. (I've used *matplotlib* to plot the graph)."
      ]
    },
    {
      "metadata": {
        "id": "z6Pfr4gB_FT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(train_costs,parameters,X_train, Y_train, X_test, Y_test):\n",
        "    #plot the graphs of training set error\n",
        "    plt.plot(np.squeeze(train_costs))\n",
        "    plt.ylabel('Cost')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.title(\"Training Set Error\")\n",
        "    plt.show()\n",
        "    L = len(parameters)//2\n",
        "    \n",
        "    #For train and test sets, perform a step of forward propagation to obtain the trained model's \n",
        "    #predictions and evaluate this\n",
        "    \n",
        "    train_cache = forward_propagation(X_train,parameters)\n",
        "    train_AL = train_cache[\"A\"+ str(L)]\n",
        "    \n",
        "    print(\"The train set MSE is: \"+str(cost_function(train_AL,Y_train)))\n",
        "        \n",
        "    test_cache = forward_propagation(X_test,parameters)\n",
        "    test_AL = test_cache[\"A\"+ str(L)]\n",
        "    \n",
        "    print(\"The test set MSE is: \"+str(cost_function(test_AL,Y_test)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qHYA2ql8kX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "Now it's time to train the model using our helper functions.\n",
        "\n",
        "Let's define our hyperparameters - I encourage you to play around with these - e.g. add more layers, change number of iterations.\n",
        "\n",
        "You might find the model does much worse on the test set - this is called **overfitting** - again you can read up more about it [here](https://mukul-rathi.github.io/2018/09/02/DebuggingLearningCurve.html)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vQwKmiqV_FT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define the hyperparameters for the model - \"tuning knobs\"\n",
        "\n",
        "num_epochs = 1500 #number of passes through the training set\n",
        "layers_units = [X.shape[0], 1] #layer 0 is the input layer - each value in list = number of nodes in that layer\n",
        "learning_rate = 1e-4 #size of our step\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "few8mbeH_FT8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "parameters, train_costs = train_model(X_train, Y_train ,num_epochs,layers_units,learning_rate)         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxRFpsk0Kd9D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluate_model(train_costs,parameters,X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2U7b7ebN9cRP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary and Extensions:\n",
        "\n",
        "You've just trained your first deep learning model! As an extension, try running the code again, but this time, use the **load_cancer()** function instead of **load boston()**. This is a dataset that classifies breast cancer as malignant/benign.\n",
        "\n",
        "Remember that sigmoid function in the lectures? We can use it to predict probabilities for classification, so all you need to do is apply it to the output of the final layer.\n",
        "\n",
        "A couple of other minor tweaks - for classification, the network uses the **cross-entropy loss** as a cost function instead of mean-square error, and you'll want to print out accuracy not MSE in the evaluation function. \n",
        "\n",
        "But the cool thing is that the network structure is the **same**! The same network, just with a sigmoid function applied to the output, can be trained on a *completely different task* and still work.\n",
        "\n",
        "That's the power of deep learning! Stay tuned for future workshops on specialised deep learning models for computer vision and natural language processing. If you want to dive deeper, head over to the [blog](http://mukul-rathi.github.io/blog.html)."
      ]
    },
    {
      "metadata": {
        "id": "SI2k1ZTt_PRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Have a good day!\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}